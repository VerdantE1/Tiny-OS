## 21：用户进程

TSS就是任务的代表，CPU用不同的TSS区分不同的任务。在CPU中有一个专门存储TSS信息的寄存器，这就是TR寄存器，它始终指向当前正在运行的任务。因此，在CPU眼里，任务切换的实质就是TR寄存器指向不同的TSS。

TSS和其他段一样，本质是一片存储数据的内存区域，Intel打算用这片内存区域保存任务的最新状态（即任务运行时占用的寄存器组，CPU一般只根据寄存器组来获取任务状态，寄存器组根据TSS获取状态），需要某个描述符结构来描述它。这就是TSS描述符，TSS描述符也要在GDT中注册，这样才能找到它。TSS描述符结构如图所示

![image-20220811152150101](D:/TYPIC/image-20220811152150101.png)

描述符的功能就是**描述一段内存区域的作用及属性，它只是对应内存区域的身份证**。 描述符的目的是为了告诉CPU描述符所对应区域的起始地址及偏移大小。同样，CPU是根据对应的寄存器而寄存器是将描述符的属性存储提供CPU。就如寄存器GDTR中的内容便是GDT的起始地址及偏移量大小。只要GDT被Lgdt指令加载后，CPU在GDTR中便能找到GDT。



TSS 描述符属于系统段描述符，因此 S 为 0，在 S 为 0 的情况下，TYPE 的值为 10B1。我们这里关注 一下 B 位，B 表示 busy 位，B 位为 0 时，表示任务不繁忙，B 位为 1 时，表示任务繁忙。



其他字段的意义与普通数据段类似，不再赘述，这里解释下什么是任务繁忙。

任务繁忙有两方面的含义，一方面就是指此任务是否为当前正在 CPU 上运行的任务。另一方面是指 此任务嵌套调用了新的任务，**CPU 正在执行新任务，此任务暂时挂起**，等新任务执行完成后 CPU 会回到 此任务继续执行，所以此任务马上就会被调度执行了。这种有嵌套调用关系的任务数不只两个，可以很多， 比如任务 A 调用了任务 A.1，任务 A.1 又调用了任务 A.1.1 等，为维护这种嵌套调用的关联，CPU 把新任 务 TSS 中的 B 位置为 1，并且在新任务的 TSS 中保存了上一级旧任务的 TSS 指针（还要把新任务标志寄存 器 eflags 中 NT 位的值置为 1），新老任务的调用关系形成了调用关系链。但是任务不能调用自己。



TSS 和 LDT 一样，必须要在 GDT 中注册才行，**这也是为了在引用描述符的阶段做安全检查**。因此 TSS 是通过选择子来访问的，将 tss 加载到寄存器 TR 的 指令是 ltr。

TR寄存器

![image-20220811153420750](D:/TYPIC/image-20220811153420750.png)



TSS段结构

![image-20220811152904972](D:/TYPIC/image-20220811152904972.png)





TSS,LDT,GDT全景图

**![image-20220811153446158](D:/TYPIC/image-20220811153446158.png)**





CPU原生任务切换的制度，现代操作系统大部分都不采用，因为非常消耗资源，我们效仿Linux的任务切换方法。Linux为每个CPU创建一个TSS，在各个CPU上的所有任务共享同一个TSS，各CPU的TR寄存器保存各CPU上的TSS，在用ltr指令加载TSS后，该TR寄存器永远只指向一个TSS，之后再也不会重新加载TSS。**在进程切换时，只需要把TSS中的SS0及esp0更新为新任务的内核栈的段地址及栈指针。**

Linux对TSS的操作是一次性加载TSS到TR，之后不断修改同一个TSS的内容，不再进行重复加载操作。Linux在TSS中只初始化了SS0，esp0和I/O位图字段，除此之外TSS便没用了，就是个空架子，不再做保存任务状态之用。

**那任务的状态保存在哪里呢？**

是这样的，当 CPU 由低特权级进入高特权级时，CPU 会“自动”从 TSS 中获取对应高特权级的栈指 针（TSS 是 CPU 内部框架原生支持的嘛，当然是自动从中获取新的栈指针）。我们具体说一下，Linux 只 用到了特权 3 级和特权 0 级，因此 CPU 从 3 特权级的用户态进入 0 特权级的内核态时（比如从用户进程 进入中断），CPU 自动从当前任务的 TSS 中获取 SS0 和 esp0 字段的值作为 0 特权级的栈，然后 Linux“手 动”执行一系列的 push 指令将任务的状态的保存在 0 特权级栈中，也就是 TSS 中 SS0 和 esp0 所指向的栈。



#### 定义初始化TSS

```c
#include "tss.h"
#include "stdint.h"
#include "global.h"
#include "string.h"
#include "print.h"

/* 任务状态段tss结构 */
struct tss {
    uint32_t backlink;
    uint32_t* esp0;
    uint32_t ss0;
    uint32_t* esp1;
    uint32_t ss1;
    uint32_t* esp2;
    uint32_t ss2;
    uint32_t cr3;
    uint32_t (*eip) (void);
    uint32_t eflags;
    uint32_t eax;
    uint32_t ecx;
    uint32_t edx;
    uint32_t ebx;
    uint32_t esp;
    uint32_t ebp;
    uint32_t esi;
    uint32_t edi;
    uint32_t es;
    uint32_t cs;
    uint32_t ss;
    uint32_t ds;
    uint32_t fs;
    uint32_t gs;
    uint32_t ldt;
    uint32_t trace;
    uint32_t io_base;
}; 
static struct tss tss;

/* 更新tss中esp0字段的值为pthread的0级线 */
void update_tss_esp(struct task_struct* pthread) {
   tss.esp0 = (uint32_t*)((uint32_t)pthread + PG_SIZE);
}



/* 创建gdt描述符 */
static struct gdt_desc make_gdt_desc(uint32_t* desc_addr, uint32_t limit, uint8_t attr_low, uint8_t attr_high) {
   uint32_t desc_base = (uint32_t)desc_addr;
   struct gdt_desc desc;
   desc.limit_low_word = limit & 0x0000ffff;
   desc.base_low_word = desc_base & 0x0000ffff;
   desc.base_mid_byte = ((desc_base & 0x00ff0000) >> 16);
   desc.attr_low_byte = (uint8_t)(attr_low);
   desc.limit_high_attr_high = (((limit & 0x000f0000) >> 16) + (uint8_t)(attr_high));
   desc.base_high_byte = desc_base >> 24;
   return desc;
}


/* 在gdt中创建tss并重新加载gdt */
void tss_init() {
   put_str("tss_init start\n");
   uint32_t tss_size = sizeof(tss);
   memset(&tss, 0, tss_size);
   tss.ss0 = SELECTOR_K_STACK;  
   tss.io_base = tss_size;

/* gdt段基址为0x900,把tss放到第4个位置,也就是0x900+0x20的位置 */

    
  /* 在gdt中添加dpl为0的TSS描述符 */
  *((struct gdt_desc*)0xc0000920) = make_gdt_desc((uint32_t*)&tss, tss_size - 1, TSS_ATTR_LOW, TSS_ATTR_HIGH);

    
  /* 在gdt中添加dpl为3的数据段和代码段描述符 */
  *((struct gdt_desc*)0xc0000928) = make_gdt_desc((uint32_t*)0, 0xfffff, GDT_CODE_ATTR_LOW_DPL3, GDT_ATTR_HIGH);
  *((struct gdt_desc*)0xc0000930) = make_gdt_desc((uint32_t*)0, 0xfffff, GDT_DATA_ATTR_LOW_DPL3, GDT_ATTR_HIGH);
   
    
    
  /* gdt 16位的limit 32位的段基址 */
   uint64_t gdt_operand = ((8 * 7 - 1) | ((uint64_t)(uint32_t)0xc0000900 << 16));   // 7个描述符大小
   asm volatile ("lgdt %0" : : "m" (gdt_operand));
   asm volatile ("ltr %w0" : : "r" (SELECTOR_TSS));
   put_str("tss_init and ltr done\n");
}

```

将TSS段信息注册到GDT中，且注册了DPL为3的数据段，DPL为3的代码段即用户栈，用户代码段。

同时要注意内核也有自己的PCB，只不过不需要注册，因为内核预留了PCB空间即0xc00009e00，而其上是有栈即内核栈0xc00009f00，主线程main执行的是内核的代码。TR指针即是CPU认为的当前任务。





#### 实现用户进程

目标是在线程的基础上实现进程。我们创建线程是通过thread_start进行的，如图

![image-20220811180837021](D:/TYPIC/image-20220811180837021.png)

 thread_start（…,function,…）的调用中，function 是我们最终在线程中执行的函数。在thread_start 内部，先是通过 get_kernel_pages(1) 在内核内存池中获取 1 个物理页做线程的 pcb，即 thread，接着调用 init_thread 初始化该线程 pcb 中的信息，然后再用 thread_create 创建线 程运行的栈，实际上是将栈中的返回地址指向了 kernel_thread 函数，因 此相当于调用了 kernel_thread，在 kernel_thread 中通过调用 function 的 方式使 function 得到执行。



经过以上的分析，如果要基于线程实现进程，我们把 function 替换 为创建进程的新函数就可以啦，先把控制权拿到手再说，进程相关的具 体工作再由新函数完成。







进程与线程最大的区别就是进程有单独的4GB空间，这指的是虚拟地址空间，物理地址未必有那么大，看似无限的虚拟地址经过分页之后最终都要落在物理地址之中。我们需要单独为每个进程维护一个虚拟地址池（IO位图），用此地址来记录该进程的虚拟空间中，哪些已被分配，哪些可以分配

```c
/* 进程或线程的pcb,程序控制块 */
struct task_struct {
   uint32_t* self_kstack;	 // 各内核线程都用自己的内核栈
   enum task_status status;
   char name[16];
   uint8_t priority;
   uint8_t ticks;	   // 每次在处理器上执行的时间嘀嗒数

/* 此任务自上cpu运行后至今占用了多少cpu嘀嗒数,
 * 也就是此任务执行了多久*/
   uint32_t elapsed_ticks;

/* general_tag的作用是用于线程在一般的队列中的结点 */
   struct list_elem general_tag;				    

/* all_list_tag的作用是用于线程队列thread_all_list中的结点 */
   struct list_elem all_list_tag;

   uint32_t* pgdir;              //          进程自己页表的虚拟地址

   struct virtual_addr userprog_vaddr;   //     用户进程的虚拟地址
   uint32_t stack_magic;	 // 用这串数字做栈的边界标记,用于检测栈的溢出
};
```

进程与线程的区别是进程拥有独立的地址空间，不同的地址空间就是不同的页表，因此我们在创建进程的过程中需要为每个进程单独创建一个页表。我们这里所说的页表是“页目录表+页表”，页目录表用来存放页目录项 PDE，每个 PDE 又指向不同的页表。

页表虽然用于管理内存，但它本身也要用内存来存储，所以要为每个进程单独申请存储页目录项及页 表项的虚拟内存页。 除此之外，咱们之前创建的线程属于内核的线程，它们运行在特权级 0。和它们相比，用户进程还多 了个特权级 3，大多数情况下，用户进程在特权级 3 下工作，因此，我们还要为用户进程创建在 3 特权级 的栈。栈也是内存区域，所以，咱们还得为进程分配内存（虚拟内存）作为 3 级栈空间。









进程创建图

![image-20220812043331962](D:/TYPIC/image-20220812043331962.png)





process_execute的参数是user_prog，这是待执行的用户进程

process_execute中，先调用函数get_kernel_page申请1页内存创建进程的PCB，这里的PCB就是thread



为了构建进程上下文，让进程在3特权级下运行



![image-20220812074626214](D:/TYPIC/image-20220812074626214.png)



构建用户进程的上下文环境，即让各种寄存器指向各种段。  进程是3级特权不能直接使用显卡段故设为0。 

设置完后将esp设置为我们填充好的中断栈proc_stack。然后jmp到intr_exit（退出中断的入口 ）进行一系列pop 寄存器使得上下文环境得到建造，之后再iretd，将eip置为当前的ip，使得指令跳转到function。 故start_process即是进程开始执行的函数调用。

这里的intr_stack只用一次，之后不需要再次访问，失去了作用。 **这样折腾的目的是因为CPU不能直接从高特权级转变为低特权级，唯一的办法即是用中断门，所以我们“假中断” 用iretd指令使得CPL从0变为了3（itred指令会用到栈中的数据作为返回地址，还会加载栈中的eflags的值到eflags寄存器）.**

```c
extern void intr_exit(void);

/* 构建用户进程初始上下文信息 */
void start_process(void* filename_) {
   void* function = filename_;
   struct task_struct* cur = running_thread();
   cur->self_kstack += sizeof(struct thread_stack);
   struct intr_stack* proc_stack = (struct intr_stack*)cur->self_kstack;	 
   proc_stack->edi = proc_stack->esi = proc_stack->ebp = proc_stack->esp_dummy = 0;
   proc_stack->ebx = proc_stack->edx = proc_stack->ecx = proc_stack->eax = 0;
   proc_stack->gs = 0;		 // 用户态用不上,直接初始为0
   proc_stack->ds = proc_stack->es = proc_stack->fs = SELECTOR_U_DATA;
   proc_stack->eip = function;	 // 待执行的用户程序地址
   proc_stack->cs = SELECTOR_U_CODE;
   proc_stack->eflags = (EFLAGS_IOPL_0 | EFLAGS_MBS | EFLAGS_IF_1);
   proc_stack->esp = (void*)((uint32_t)get_a_page(PF_USER, USER_STACK3_VADDR) + PG_SIZE) ;
   proc_stack->ss = SELECTOR_U_DATA; 
   asm volatile ("movl %0, %%esp; jmp intr_exit" : : "g" (proc_stack) : "memory");
}

```







每个进程都拥有自己的独立虚拟地址空间，本质上就是各个进程拥有自己单独的页表，页表是存储再页表寄存器CR3中的，CR3寄存器只有一个，因此，不同的进程在执行前，我们要在CR3寄存器中为其换上与之配套的页表，从而实现了虚拟地址空间的隔离。

线程并不是为用户进程服务的，它是为内核服务的，因此与内核共享同一地址空间，所以当切换到线程时，必须将页表更换为内核所使用的页表。

```c
/* 更换且激活页表 */
void page_dir_activate(struct task_struct* p_thread) {
/********************************************************
 * 执行此函数时,当前任务可能是线程。
 * 之所以对线程也要重新安装页表, 原因是上一次被调度的可能是进程,
 * 否则不恢复页表的话,线程就会使用进程的页表了。
 ********************************************************/

/* 若为内核线程,需要重新填充页表为0x100000 */
   uint32_t pagedir_phy_addr = 0x100000;  // 默认为内核的页目录物理地址,也就是内核线程所用的页目录表
   if (p_thread->pgdir != NULL)	{    // 用户态进程有自己的页目录表
      pagedir_phy_addr = addr_v2p((uint32_t)p_thread->pgdir);
   }
   /* 更新页目录寄存器cr3,使新页表生效 */
   asm volatile ("movl %0, %%cr3" : : "r" (pagedir_phy_addr) : "memory");
}

```





激活线程或进程的页表 且 更换tss中的esp0为进程的特权级0的栈。进程与线程都是独立的执行流，它们有各自的栈和页表，只不过线程的页表和其他线程共用，而进程的页表是单独的。**进程或线程在被中断信号打断时，处理器会进入0特权级，并会在0特权级中保存进程或线程的上下文环境。如果当前被中断的是3特权级的用户进程，处理器会自动到tss中获取esp0的值作为用户进程在内核态（0特权级）的栈地址**，如果被中断的是0特权级的内核线程，由于内核线程已是0特权级，进入中断后不涉及特权级的改变，所以处理器并不会到TSS中获取esp0，**言外之意即是即使咱们更新了线程tss中的esp0，处理器也不会使用它，处理器并不会去获取TSS中的esp0.**

```c
void process_activate(struct task_struct* p_thread) {
   ASSERT(p_thread != NULL);
   /* 激活该进程或线程的页表 */
   page_dir_activate(p_thread);

   /* 内核线程特权级本身就是0,处理器进入中断时并不会从tss中获取0特权级栈地址,故不需要更新esp0 */
   if (p_thread->pgdir) {
      /* 更新该进程的esp0,用于此进程被中断时保留上下文 */
      update_tss_esp(p_thread);
   }
}
```



用户栈是用户进程空间中的一块区域，用于保存用户进程的子程序间相互调用的参数、返回值、返回点以及子程序(函数)的局部变量。  

****

在 4GB 的虚拟地址空间中，(0xc0000000-1)是用户空间的最高地址，0xc0000000～0xffffffff 是内核空间。但它 们相当于位于栈的最高地址处，所以用户栈的栈底地址为 0xc0000000。这里我们用宏来定义此地址，即 USER_STACK3_VADDR， 它定义在 userprog.h 中。 “#define USER_STACK3_VADDR (0xc0000000 - 0x1000)”。**这里的0xc0000000并不是内核的页目录表对应的虚拟地址，而是每个进程各自通过creat_page_dir创建的页目录表和页表且通过process_activate更改CR3指针加载生效的。**







各进程虚拟地址空间都是规划的，若需使用则申请一片不同的物理内存地址与之关联。相同的地址（在用户空间内非内核虚拟空间）在不同的进程所对应的物理地址是不一样的。

![image-20220812083012138](D:/TYPIC/image-20220812083012138.png)

这也证明了内核栈（0级栈）是共享的，3级栈是各个进程各有的。 

同一个用户程序在被加载多次时，操作系统每次为它们分配的物理内存也不会固定，但操作系统不一样，操作系统是通过汇编固定规划的地址，其在物理内存中的物理地址是固定的，不会再变动。我们的操作系统就是为了用户进程服务，所以我们得让用户进程能够访问到内核，我们使用内存共享，其实就是将内核所在的页目录项和页表的内容复制到进程页目录表中同等的位置，这样就能让用户进程的高1GB空间指向内核。

最后要让页目录项最后一个指向自己。

```c
uint32_t* create_page_dir(void) {

   /* 用户进程的页表不能让用户直接访问到,所以在内核空间来申请 */
   uint32_t* page_dir_vaddr = get_kernel_pages(1);
   if (page_dir_vaddr == NULL) {
      console_put_str("create_page_dir: get_kernel_page failed!");
      return NULL;
   }

/************************** 1  先复制页表  *************************************/
   /*  page_dir_vaddr + 0x300*4 是内核页目录的第768项 */
   memcpy((uint32_t*)((uint32_t)page_dir_vaddr + 0x300*4), (uint32_t*)(0xfffff000+0x300*4), 1024);
/*****************************************************************************/

/************************** 2  更新页目录地址 **********************************/
   uint32_t new_page_dir_phy_addr = addr_v2p((uint32_t)page_dir_vaddr);
   /* 页目录地址是存入在页目录的最后一项,更新页目录地址为新页目录的物理地址 */
   page_dir_vaddr[1023] = new_page_dir_phy_addr | PG_US_U | PG_RW_W | PG_P_1;
/*****************************************************************************/
   return page_dir_vaddr;
}

```





现在用户进程的虚拟地址空间已构造（即页表页目录项建立），内核地址已复制，那么剩下的用户虚拟空间需要分配，所以我们需要一个内存池进行管理

```c
void create_user_vaddr_bitmap(struct task_struct* user_prog) {
   user_prog->userprog_vaddr.vaddr_start = USER_VADDR_START;
   uint32_t bitmap_pg_cnt = DIV_ROUND_UP((0xc0000000 - USER_VADDR_START) / PG_SIZE / 8 , PG_SIZE);
   user_prog->userprog_vaddr.vaddr_bitmap.bits = get_kernel_pages(bitmap_pg_cnt);
   user_prog->userprog_vaddr.vaddr_bitmap.btmp_bytes_len = (0xc0000000 - USER_VADDR_START) / PG_SIZE / 8;
   bitmap_init(&user_prog->userprog_vaddr.vaddr_bitmap);
}

```



最后便是集大成，进程创建，创建完后加入就绪队列，全部队列,等待调度。

```cc
void process_execute(void* filename, char* name) { 
   /* pcb内核的数据结构,由内核来维护进程信息,因此要在内核内存池中申请 */
   struct task_struct* thread = get_kernel_pages(1);
   init_thread(thread, name, default_prio); 
   create_user_vaddr_bitmap(thread);
   thread_create(thread, start_process, filename);
   thread->pgdir = create_page_dir();
   
   enum intr_status old_status = intr_disable();
   ASSERT(!elem_find(&thread_ready_list, &thread->general_tag));
   list_append(&thread_ready_list, &thread->general_tag);

   ASSERT(!elem_find(&thread_all_list, &thread->all_list_tag));
   list_append(&thread_all_list, &thread->all_list_tag);
   intr_set_status(old_status);
}
```





调度开始

```c
void schedule() {

   ASSERT(intr_get_status() == INTR_OFF);

   struct task_struct* cur = running_thread(); 
   if (cur->status == TASK_RUNNING) { // 若此线程只是cpu时间片到了,将其加入到就绪队列尾
      ASSERT(!elem_find(&thread_ready_list, &cur->general_tag));
      list_append(&thread_ready_list, &cur->general_tag);
      cur->ticks = cur->priority;     // 重新将当前线程的ticks再重置为其priority;
      cur->status = TASK_READY;
   } else { 
      /* 若此线程需要某事件发生后才能继续上cpu运行,
      不需要将其加入队列,因为当前线程不在就绪队列中。*/
   }

   ASSERT(!list_empty(&thread_ready_list));
   thread_tag = NULL;	  // thread_tag清空
/* 将thread_ready_list队列中的第一个就绪线程弹出,准备将其调度上cpu. */
   thread_tag = list_pop(&thread_ready_list);   
   struct task_struct* next = elem2entry(struct task_struct, general_tag, thread_tag);
   next->status = TASK_RUNNING;

   /* 击活任务页表等 */
   process_activate(next);   //这里要激活页表，切换TSS的ESP0

   switch_to(cur, next);
}
```









最后说几点：

1.进程的运行必备“材料”保存在内存中，如全局变量，静态变量等；上下文环境保存在3级栈中，如局部变量等。

2.所谓的动态内存申请，是指程序在运行的过程中申请的内存，并不是在程序加载时由操作系统加载器为程序段分配的“固定，静态”内存，这种动态申请是操作系统从申请者自己的堆中分配的。

3.用户程序的装载本质就是将用户程序中类型为LOAD的段拷贝到指定的内存地址。

4.操作系统会根据程序中一些数据的属性去归纳为一个段即Segment，然后再根据不同的段进行分配内存。

5.在C程序的内存空间中，位于低处的三个段是代码段，数据段和BSS段，它们由编译器和链接器规划地址空间，在程序被操作系统加载之前它们的地址就已经固定了。